{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "153213ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse as sc\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f9d1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Données de tests complètes\n",
    "test_inputs = df_test.iloc[:, :-1]\n",
    "test_labels = df_test.iloc[:, -1:]\n",
    "\n",
    "def split_training_data(df):\n",
    "    train_set = []\n",
    "    valid_set = []\n",
    "    \n",
    "    for (i,row) in enumerate(df):\n",
    "        if i % 5 < 4:\n",
    "            train_set.append(list(df))\n",
    "        else:\n",
    "            valid_set.append(list(df))\n",
    "    \n",
    "    return (np.array(train_set), np.array(valid_set))\n",
    "\n",
    "\n",
    "# Données complètes\n",
    "X_columns = [c for c in df_train.columns if c not in ['S.No', 'LABELS']]\n",
    "train_inputs = df_train.loc[:, X_columns]\n",
    "train_labels = df_train.loc[:, 'LABELS']\n",
    "\n",
    "# On trie le training set\n",
    "df_train_sorted = df_train.sort_values(by=['LABELS'], axis=0, ascending=False)\n",
    "df_train_light = df_train_sorted.iloc[5000:13000, :].sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Données sélectionné un peu plus égal pour chaque classe pour 1 seule grande batch\n",
    "train_inputs_light = df_train_light.iloc[:, :-1]\n",
    "train_labels_light = df_train_light.iloc[:, -1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0b819d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_split_training_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19060/247332035.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_split_training_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train_light\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_split_training_data' is not defined"
     ]
    }
   ],
   "source": [
    "train_set, valid_set = train_split_training_data(df_train_light)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36c5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet l'encodage onehot de n'importe quel scalaire n_class est simplement le nombre de classe qui va\n",
    "# donc nous donner la longueur que le vecteur final aura.\n",
    "def onehot(x,n_class):\n",
    "    encoded = []\n",
    "    for i in x:\n",
    "        yi_encoded = np.zeros(n_class)\n",
    "        yi_encoded[int(i)] = 1\n",
    "        encoded.append(yi_encoded)\n",
    "    return np.array(encoded)\n",
    "\n",
    "# Prends un vecteur et retourne un vecteur où a été appliqué la fonction softmax sur tous les éléments du vecteur x.\n",
    "def softmax_vect(x):\n",
    "    y = np.exp(x-np.max(x))\n",
    "    div = np.sum(y)\n",
    "    return y/div\n",
    "\n",
    "# Retourne la proportion de bonne prédictions trouver avec un certain estimateur. y_pred et y_vrai ne doivent pas être\n",
    "# encodé en onehot.\n",
    "def accuracy(y_pred, y_vrai):\n",
    "    somme = 0\n",
    "    for i in range(len(y_vrai)):\n",
    "        if np.array_equal(y_vrai[i],y_pred[i]):\n",
    "            somme+=1\n",
    "    return somme / len(y_vrai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eead9891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "\n",
    "    def __init__(self, y_labels, x_train):\n",
    "        self.y_labels = y_labels\n",
    "        self.x_train = x_train\n",
    "        self.nb_classe = len(np.unique(y_labels))\n",
    "\n",
    "    # Cette fonction va trouver le w et le b qui sont optimals.\n",
    "    def train(self, eta, lam, nb_iterations):\n",
    "\n",
    "        w_current = np.random.random([len(x_train[1, :]), 3])\n",
    "        b_current = np.random.random(1)\n",
    "        y_onehot = onehot(self.y_labels, self.nb_classe)\n",
    "\n",
    "        for i in range(nb_iterations):\n",
    "            z = np.dot(x_train, w_current) + b_current\n",
    "            y_hat = np.array([softmax_vect(x) for x in z])\n",
    "            w_grad = (1 / len(self.x_train[:, 0])) * np.dot(np.transpose(self.x_train), (y_hat - y_onehot)) + 2 * lam * w_current\n",
    "            b_grad = (1 / len(self.x_train[:, 0])) * np.sum(y_hat - y_onehot)\n",
    "            w_current = w_current - eta * w_grad\n",
    "            b_current = b_current - eta * b_grad\n",
    "            self.w = w_current\n",
    "            self.b = b_current\n",
    "        self.w = w_current\n",
    "        self.b = b_current\n",
    "\n",
    "    # Fonction qui retourne un vecteur contenant les classes de toutes les prédictions.\n",
    "    def predict(self, x_test):\n",
    "        z = np.dot(x_test, self.w) + self.b\n",
    "        y_onehot = onehot(self.y_labels, self.nb_classe)\n",
    "        y_hat = np.array([softmax_vect(x) for x in z])\n",
    "        class_pred = [np.argmax(y_hat[i]) for i in range(len(y_hat))]\n",
    "        return class_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d3572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(modelclass, w0=[-3.0, 3.0, 0.1], reg=.1, stepsize=.2, plot=False):\n",
    "    \"\"\"Crée une instance de modelclass, entraîne la, calcule le taux d'erreurs sur un\n",
    "    test set, trace les courbes d'apprentissage et la frontieres de decision.\n",
    "    \"\"\"\n",
    "    model = modelclass(w0, reg)\n",
    "    training_loss, training_error = model.train(trainset, stepsize, 100, plot=plot)\n",
    "    print(\"The test error is {:.2f}%\".format(\n",
    "      model.test(testset[:,:-1], testset[:,-1])*100))\n",
    "    print('Initial weights: ', w0)\n",
    "    print('Final weights: ', model.w)\n",
    "    print(\"Last train loss\", training_loss.mean())\n",
    "\n",
    "    # learning curves\n",
    "    fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(8,2))\n",
    "    ax0.plot(training_loss)\n",
    "    ax0.set_title('loss')\n",
    "    ax1.plot(training_error)\n",
    "    ax1.set_title('error rate')\n",
    "\n",
    "    # data plot\n",
    "    plt.figure()\n",
    "    scatter(trainset, marker='x')\n",
    "    scatter(testset, marker='^')\n",
    "    decision_boundary(model.w)\n",
    "    finalize_plot(modelclass.__name__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
